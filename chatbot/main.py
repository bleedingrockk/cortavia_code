from langchain_core.prompts import ChatPromptTemplate
from .llm import llm
from pydantic import BaseModel, Field

# Define the expected output structure using Pydantic
class TranslationOutput(BaseModel):
    """
    Schema for the LLM response.

    Attributes:
        response (str): The response generated by the LLM.
    """
    response: str = Field(description="Conversation response")

# Create an LLM chain with structured output based on the defined schema
conversation_llm = llm.with_structured_output(TranslationOutput)

def interact_with_llm(user_input: str) -> TranslationOutput:
    """
    Sends user input to the LLM and returns a structured response.

    Args:
        user_input (str): The text input from the user.

    Returns:
        TranslationOutput: The structured output containing the LLM's response.
    """
    response = conversation_llm.invoke(user_input).response
    print(f"For user input: {user_input} Generated answer {response}" )
    return response

if __name__ == "__main__":
    # Simple input loop
    print("Chat with Gemini! Type 'exit' to quit.\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            break
        response = interact_with_llm(user_input)
        print("Gemini:", response.response)